<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Document</title>
    <script src="https://cdn.jsdelivr.net/npm/handtrackjs/dist/handtrack.min.js"> </script>

</head>

<body>


    <video class="videobox canvasbox" autoplay="autoplay" id="myvideo" width="640" height="480"></video>
    <canvas id="canvas" class="border canvasbox"></canvas>
    <br>
    <button id="trackbutton" type="button">
        Toggle Video
    </button>

    <!-- Place your code in the script tag below. You can also use an external .js file -->
    <script>
        const video = document.getElementById("myvideo");
        const canvas = document.getElementById("canvas");
        const context = canvas.getContext("2d");
        let trackButton = document.getElementById("trackbutton");
        let nextImageButton = document.getElementById("nextimagebutton");

        let imgindex = 1
        let isVideo = false;
        let model = null;

        const modelParams = {
            flipHorizontal: true, // flip e.g for video  
            maxNumBoxes: 2, // maximum number of boxes to detect
            iouThreshold: 0.5, // ioU threshold for non-max suppression
            scoreThreshold: 0.6, // confidence threshold for predictions.
        }
        handTrack.load(modelParams).then(lmodel => {
            // detect objects in the image.
            model = lmodel

        });
        trackButton.addEventListener("click", function () {
            toggleVideo();
        });

        function startVideo() {
            handTrack.startVideo(video).then(function (status) {
                console.log("video started", status);
                if (status) {
                    isVideo = true
                    video.style.display = "none";
                    runDetection()
                }
            });
        }

        function toggleVideo() {
            if (!isVideo) {
                startVideo();
            } else {
                handTrack.stopVideo(video)
                isVideo = false;
            }
        }

        function runDetection() {
            model.detect(video).then(predictions => {
                if (predictions.length > 0) console.log("Predictions: ", predictions[0].bbox[0], predictions[0].bbox[1]);

                model.renderPredictions(predictions, canvas, context, video);
                if (predictions.length > 0 && predictions[0].bbox[0] >= 320 && predictions[0].bbox[1] <= 240) {
                    context.fillStyle = "green";
                    context.fillRect(320, 0, 320, 240);
                }
                if (isVideo) {
                    requestAnimationFrame(runDetection);



                }
            });
        }
    </script>
</body>

</html>