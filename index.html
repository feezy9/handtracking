<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Document</title>
    <script src="https://cdn.jsdelivr.net/npm/handtrackjs/dist/handtrack.min.js"> </script>

</head>

<body>

  
    <video class="videobox canvasbox" autoplay="autoplay" id="myvideo"></video>
    <canvas id="canvas" class="border canvasbox"></canvas>
    <br>
    <button id="trackbutton" type="button">
        Toggle Video
    </button>

    <!-- Place your code in the script tag below. You can also use an external .js file -->
    <script>
        const video = document.getElementById("myvideo");
        const canvas = document.getElementById("canvas");
        const context = canvas.getContext("2d");
        let trackButton = document.getElementById("trackbutton");
        let nextImageButton = document.getElementById("nextimagebutton");

        let imgindex = 1
        let isVideo = false;
        let model = null;

        const modelParams = {
            flipHorizontal: true, // flip e.g for video  
            maxNumBoxes: 2, // maximum number of boxes to detect
            iouThreshold: 0.5, // ioU threshold for non-max suppression
            scoreThreshold: 0.6, // confidence threshold for predictions.
        }

        trackButton.addEventListener("click", function () {
            toggleVideo();
        });

        function startVideo() {
            handTrack.startVideo(video).then(function (status) {
                console.log("video started", status);
                if (status) {
                    isVideo = true
                    video.style.display = "none";
                    runDetection()
                }
            });
        }

        function toggleVideo() {
            if (!isVideo) {
                startVideo();
            } else {
                handTrack.stopVideo(video)
                isVideo = false;
            }
        }

        function runDetection() {
            model.detect(video).then(predictions => {
                console.log("Predictions: ", predictions);
                model.renderPredictions(predictions, canvas, context, video);
                if (isVideo) {
                    requestAnimationFrame(runDetection);
                }
            });
        }

        function runDetectionImage(img) {
            model.detect(img).then(predictions => {
                console.log("Predictions: ", predictions);
                model.renderPredictions(predictions, canvas, context, img);
            });
        }
        handTrack.load(modelParams).then(lmodel => {
            // detect objects in the image.
            model = lmodel
            runDetectionImage(handimg)
        });
    </script>
</body>

</html>